Bonjour ! Je m’appelle Carrie Anne, vous regardez Crash Course Informatique,

et aujourd'hui, nous allons parler de comment les ordinateurs stockent et représentent les données numériques.

Ce qui veut dire qu'on va parler de maths !

Mais pas d'inquiétude.

Chacun d'entre vous en sait déjà suffisamment pour suivre.

Dans le dernier épisode, nous avons parlé de comment les transistors permettent de construire des fonctions logiques,

capable d'évaluer des expressions booléennes.

Et en algèbre de Boole, il n'existe que deux valeurs binaires : vrai et faux.

Mais comment, avec uniquement deux valeurs, représenter l'information

au delà de ces deux valeurs ?

C'est là qu'on passe aux maths.

* Générique d'introduction *

Comme nous l'avons dit au dernier épisode, une unique valeur binaire peut représenter un nombre.

Au lieu de vrai et faux, on appelle ces deux états 1 et 0, ce qui facilite énormément les choses.

Et si nous voulons représenter des plus grands nombres, il faut juste ajouter plus de chiffres binaires.

Ca marche exactement de la même façon que les nombres décimaux que l'on connait tous.

Avec les nombres décimaux, il n'y a « que » 10 valeurs possibles pour un chiffre, de 0 à 9,

et pour dépasser 9, on ajoute plus de chiffres à l'avant.

On fait la même chose en binaire.

Par exemple, prenons deux cent soixante trois.

Qu'est ce que ce nombre représente, exactement ?

Eh bien, cela veut dire que nous avons 2 centaines, 6 dizaines, et 3 unités.

En les ajoutant ensemble, on obtient 263.

Remarquez que chaque colonne a un multiplicateur différent.

Dans ce cas, il s'agit de 100, 10 et 1.

Chaque multiplicateur est dix fois plus grand que celui à droite.

C'est parce que chaque colonne peut représenter dix chiffres, de 0 à 9,

après quoi il faut retenir 1 à la colonne suivante.

C'est pour cette raison qu'on l'appelle notation en base 10, ou décimale puisque « deci » veut dire 10.

Et le binaire fonctionne exactement de la même façon, en base 2.

Parce qu'il n'y a que deux chiffres possibles en binaire, 1 et 0.

Cela signifie que chaque multiplicateur est deux fois plus grand que celui dans la colonne de droite.

Plutôt que des centaines, dizaines et unités, on a des quatraines, deuxaines et unités.

Par exemple, si on prend le nombre binaire 101.

Cela signifie qu'il y a 1 fois quatre, 0 fois deux et 1 fois un.

En les additionnant, on obtient 5 en base 10.

Mais pour représenter des nombres plus grands, le binaire à besoin de beaucoup, beaucoup de chiffres.

Prenons ce nombre binaire : 10110111.

On peut le convertir en décimal de la même façon.

Ca nous donne 1 x 128, 0 x 64, 1 x 32, 1 x 16, 0 x 8, 1 x 4, 1 x 2, et 1 x 1.

Ce qui donne 183.

Les maths en binaire ne sont pas difficiles non plus.

Par exemple, si on additionne 183 et 19.

D'abord, on calcule 3+9, soit 12, donc on pose 2 comme résultat et on retient 1 pour la colonne des dizaines.

Maintenant, on ajoute 8 plus 1 plus la retenue,
ça nous donne 10, donc on pose 0 et on retient 1.

Enfin, on ajoute 1 et la retenue, pour obtenir 2.

Donc la somme totale est 202.

Voilà la même somme en binaire.

Comme avant, on commence par la colonne des unités.

1+1 donne 2, même en binaire.

Mais il n'existe pas de symbole « 2 », donc on utilise 10 : on pose 0 et on retient 1.

Comme dans l'exemple décimal.

1 plus 1 plus la retenue donne 3, ou 11 en binaire, donc on pose 1

et on retient 1, et ainsi de suite.

A la fin, on obtient 11001010, qui équivaut au nombre 202 en base 10.

Chacun de ces chiffres binaires, 1 ou 0, est appelé un « bit ».

Dans les exemples précédents, on utilisait des nombres de 8 bits avec leur valeur la plus faible étant 0

et valeur la plus haute étant 255, qui nécessite que les 8 bits soient 1.

Ca représente 256 valeurs différentes, ou 2^8.

Vous avez peut-être entendu parler des ordinateurs 8 bits, ou des graphismes ou son 8 bits.

C'était des ordinateurs qui faisaient la plupart de leurs opérations en morceaux de 8 bits.

Mais 256 valeurs différentes n'est pas énorme
pour calculer, donc cela voulait dire que les jeux vidéo 8 bits

étaient limités à des graphismes à 256 couleurs.

En fait, 8 bits est une taille si commune en informatique qu'elle a un nom spécifique : un octet.

Un octet correspond à 8 bits.

Si vous avez 10 octets, vous avez en réalité 80 bits.

Vous avez déjà entendu parler des kilo-octets, mega-octets, mega-octets et ainsi de suite.

Ces préfixes dénotent différentes quantités de données.

De la même façon q'un kilogramme correspond à mille grammes, un kilo-octet correspond à mille octets...

ou en vérité, 8000 bits.

Mega correspond a un million d'octets (Mo) et giga correspond à un milliard d'octets (Go).

De nos jours, il se peut même que vous ayez un disque dur d'un tera-octet (To) de stockage.

C'est un billion de 0 et 1.

Mais pas si vite !

Ce n'est pas toujours vrai.

En binaire, un kilo-octet a 2^10 octets, soit 1024.

Quand on parle de kilo-octets, 1000 octets est juste aussi, mais il faut se rendre compte que ce n'est pas

la seule définition valable.

Vous avez aussi sans doute entendu parler des ordinateurs 32 bits ou 64 bits...

il y a de grandes chances que vous en utilisez un en ce moment.

Ca signifie qu'ils fonctionnent avec des morceaux de 32 ou 64 bits.

C'est beaucoup de bits !

Le plus grand nombre qu'il est possible de représenter avec 32 bits est légèrement plus petit que 4,3 milliards.

Soit trente deux 1 à la suite en binaire.

C'est pour ça que les photos Instagram sont si lisses et jolies, elles sont composées

de millions de couleurs, parce que les ordinateurs d'aujourd'hui utilisent des graphismes 32 bits.

Bien sûr, il n'y a pas que les nombres positifs (par exemple, mon compte bancaire à l'université)

Il nous faut donc un moyen de représenter des nombres positifs et négatifs.

La plupart des ordinateurs utilisent le premier bit pour le signe : 1 pour négatif, 0 pour positif,

et le reste des 31 bits est utilisé pour le nombre lui-même.

Ca nous donne grossièrement une fourchette de mois deux milliards à deux milliards.

C'est une gamme de nombres impressionnante, mais ce n'est pas assez pour de nombreuses tâches.

Il y a sept milliards d'humains sur Terre, et la dette nationale américaine est de 20 billions de dollars, après tout.

Les nombres 64 bits règlent ce problème.

La plus grande valeur représentable en 64 bits est d'environ 9,2 quadrillions !

Ca fait beaucoup de nombres, et ça restera toujours bien au dessus de la dette nationale américaine, espérons.

Plus important encore, comme nous allons l'aborder dans un futur épisode, les ordinateurs doivent étiqueter des emplacements

dans leur mémoire, appelées adresses, pour stocker et récupérer des valeurs.

La mémoire informatique ayant évolué jusqu'aux giga-octets et tera-octets, soit des billions d'octets,

il a fallu également créer des adresses mémoires 64 bits.

En plus des nombres positifs et négatifs, les ordinateurs travaillent

avec des nombres non-entiers, comme 12,7 ou 3,14, ou même des dates stellaires, comme 43989,1.

Ils sont appelés les « nombres à virgule flottante », parce que la virgule peut flotter

au beau milieu du nombre.

Plusieurs méthodes ont été développées pour représenter des nombres à virgule flottante.

La plus courante d'entre elles est la norme IEEE 754.

Et vous pensiez que les historiens étaient les seuls à nommer les choses n'importes comment !

Fondamentalement, cette norme stocke les valeurs décimales un peu comme la notation scientifique.

Par exemple, 625,9 peut être écrit sous la forme 0,6259 x 10^3.

Il y a deux nombres importants ici : 0,6259 est appelé la mantisse.

Et 3 est l'exposant.

Dans un nombre à virgule flottante 32 bits, le premier bit est utilisé pour le signe du nombre,

positif ou négatif.

Les 8 bits suivants sont utilisés pour stocker l'exposant et les 23 bits suivants,

pour stocker la mantisse.

Ok, on a beaucoup parlé de nombres, mais votre nom est probablement composé de lettres,

donc il est vraiment utile pour les ordinateurs de
savoir représenter du texte.

Cependant, plutôt que d'avoir un type de stockage spécifique pour les lettres,

les ordinateurs utilisent des nombres pour représenter les lettres.

L'approche la plus simple serait de numéroter les lettres de l'alphabet :

A serait 1, B serait 2, C serait 3, et ainsi de suite.

D'ailleurs, Francis Bacon, le célèbre écrivain anglais, utilisait des séquences de 5 bits pour encoder

les 26 lettres de l'alphabet anglais pour envoyer des messages secrets dans les années 1600.

Et cinq bits peuvent stocker 32 valeurs, ce qui est suffisant pour les 26 lettres,

mais pas assez pour la ponctuation, les nombres, 
les minuscules et les majuscules.

Puis vint ASCII, l'American Standard Code for Information Interchange [Code américain normalisé pour l'échange d'information]

Inventé en 1963, ASCII était un code de 7 bits, assez pour stocker 128 valeurs différentes.

Avec cette gamme élargie, il pouvait coder les majuscules, les lettres minuscules,

les chiffres de 0 à 9, les symboles comme le signe @ et les marques de ponctuation.

Par exemple, le 'a' minuscule est représenté par le nombre 97, tandis que le 'A' majuscule est représenté par par 65.

Deux points est 58, et la parenthèse fermante est 41.

ASCII avait même une sélection de codes de commande spéciaux, comme un caractère de saut de ligne pour indiquer

à l'ordinateur où terminer une ligne et passer à la suivante.

Dans les systèmes informatiques plus anciens, la ligne de texte continue littéralement

au-delà de la limite de l'écran si vous oubliez le caractère de saut de ligne !

Vu qu'il est arrivé si tôt, ASCII a été largement adopté,

il a permis à différents ordinateurs conçus par des entreprises différentes d'échanger des données, ce qui a été fondamental.

On appelle « interopérabilité » cette capacité d'échanger l'information de façon universelle.

Cependant, il y avait une limitation majeure : le système avait été conçu uniquement pour l'anglais.

Heureusement, il y a 8 bits dans un octet, et pas 7, et tout le monde s'est vite mis à utiliser les codes précédemment inutilisés,

de 128 à 255, pour les caractères « nationaux ».

Aux États-Unis, ces nombres supplémentaires ont été utilisés pour coder des symboles additionnels, comme la notation

scientifique, les éléments graphiques, et les caractères accentués communs.

D'autre part, tandis que les caractères latins ont tous été utilisés, les ordinateurs russes

ont utilisé les codes supplémentaires pour encoder les caractères cyrilliques, et les ordinateurs grecs ont fait de même avec les lettres grecques, et ainsi de suite.

Les codes de caractères nationaux fonctionnaient assez bien pour la plupart des pays.

Le problèm, c'est que si vous ouvreiz un e-mail écrit en letton sur un ordinateur turc,

le résultat était complètement incompréhensible.

Et les problèmes ont continué avec la montée de l'informatique en Asie, étant donné que les languages comme le chinois et japonais

ont des milliers de caractères.

Il n'y avait aucune façon d'encoder tous ces caractères avec 8 bits !

En réponse à cela, chaque pays a inventé des systèmes de codages à plusieurs octets, tous incomptatibles les uns avec les autres.

Les Japonais étaient si familiers avec ce problème d'encodage qu'ils lui ont donné un nom :

« mojibake », ce qui signifie « changement de caractère ».

Ainsi donc naquit Unicode, un standard pour les gouverner tous.

Conçu en 1992 pour faire enfin disparaître tous les différents systèmes internationaux,

il les a remplacé avec un seul système de codage universel.

La version la plus commune d'Unicode utilise 16 bits, assez d'espace pour plus d'un million de codes,

suffisamment pour chaque caractère de chaque langue jamais parlée,

plus de 120 000 d'entre eux dans plus de 100 types de script,

Et aussi l'espace pour les symboles mathématiques et même des caractères graphiques comme les émoticônes.

Et de la même manière que ASCII définit un système de codage des lettres en tant que nombres binaires,

d'autres formats de fichiers, comme les MP3 ou les gifs,

utilisent les nombres binaires pour encoder des sons ou les couleurs d'un pixel dans nos photos, films et musiques.

Sous le capot, tout n'est que de longues séquences de bits.

Les messages texte, cette vidéo YouTube, chaque page Web sur Internet,

et même le système d'exploitation de votre ordinateur, ne sont que des longues séquences de 0 et de 1.

La semaine prochaine, nous allons commencer à parler de comment votre ordinateur

manipule ces séquences binaires, pour notre premier vrai plongeon dans la computation.

Merci d’avoir regardé. A la semaine prochaine.

